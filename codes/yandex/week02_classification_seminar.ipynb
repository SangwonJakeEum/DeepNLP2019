{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large scale text analysis with deep learning (3 points)\n",
    "\n",
    "Today we're gonna apply the newly learned tools for the task of predicting job salary.\n",
    "\n",
    "<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/3342/media/salary%20prediction%20engine%20v2.png\" width=400px>\n",
    "\n",
    "_Special thanks to [Oleg Vasilev](https://github.com/Omrigan/) for the core assignment idea._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the challenge\n",
    "For starters, let's download and unpack the data from [here](https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=0). \n",
    "\n",
    "You can also get it from [yadisk url](https://yadi.sk/d/vVEOWPFY3NruT7) the competition [page](https://www.kaggle.com/c/job-salary-prediction/data) (pick `Train_rev1.*`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!curl -L https://www.dropbox.com/s/5msc5ix7ndyba10/Train_rev1.csv.tar.gz?dl=1 -o Train_rev1.csv.tar.gz\n",
    "#!tar -xvzf ./Train_rev1.csv.tar.gz\n",
    "data = pd.read_csv(\"./Train_rev1.csv\", index_col=None)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with salary prediction is that it's oddly distributed: there are many people who are paid standard salaries and a few that get tons o money. The distribution is fat-tailed on the right side, which is inconvenient for MSE minimization.\n",
    "월급이 기이하게 분포 되어 있다.\n",
    "표준 급여를 받는 사람이 많다. MSE 최소화에 불편한 분포가 오른쪽 측면에서 꼼꼼하게 나타납니다.\n",
    "\n",
    "There are several techniques to combat this: using a different loss function, predicting log-target instead of raw target or even replacing targets with their percentiles among all salaries in the training set. We gonna use logarithm for now.\n",
    "이를 해결하기위한 여러 가지 기술이 있습니다. 즉, 다른 손실 함수 사용, 원시 타겟 대신 로그 타겟을 예측하거나 트레이닝 세트의 모든 급여 중 목표를 백분위 수로 대체하는 것입니다. 우리는 대수(대신하는수)를 사용할 것입니다.\n",
    "\n",
    "_You can read more [in the official description](https://www.kaggle.com/c/job-salary-prediction#description)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAAD8CAYAAACFB4ZuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHpNJREFUeJzt3X+sZ3V95/HnS34orSKDDIQwsIPtpJWSiHgXpnFjLHRhwKaDG21wmzJ1SabrQoNJd9exbYL1xwZ3U1nZWrrTMmUwVmBR40QHx1nEdU3kx6AjPxwtI7IyZZYZO4gYUlzwvX+cz5Uvw/fe+7137twf5z4fyTff832fzznfz/ne77nv7znncz6fVBWSJKkfXjbfFZAkSbPHxC5JUo+Y2CVJ6hETuyRJPWJilySpR0zskiT1iIldkqQeMbFLktQjJnZJknrkyPmuwEydcMIJtXLlyvmuhrSg3XfffT+squXzXY/JuC9Loxl1f160iX3lypXs2LFjvqshLWhJ/s9812Eq7svSaEbdnz0VL0lSj5jYJUnqERO7JEk9YmKXJKlHTOySJPWIiV2SpB4xsUuS1CMmdkmSesTELklSjyzanudmy8oNX5iyzKPXvHUOaiJJC4f/GxevJZ/YR+EXXJK0WHgqXpKkHjGxS5LUIyZ2SZJ6xMQuSVKPmNglSeoRE7u0RCR5RZJ7knwryUNJ/qzFb0zy/SQ72+OsFk+S65LsTnJ/krMH1rUuycPtsW4g/sYkD7RlrkuSud9SaWnzdjdp6XgWOK+qfpLkKOBrSW5v8/5DVd12UPmLgFXtcS5wPXBukuOBq4ExoID7kmypqidbmfXAXcBWYA1wO1owRrl9V4ubR+zSElGdn7SXR7VHTbLIWuCmttxdwHFJTgYuBLZX1YGWzLcDa9q8Y6vq61VVwE3AJYdtgyQNZWKXlpAkRyTZCeyjS853t1kfbqfbr03y8hY7BXhsYPE9LTZZfM+QuKQ5NFJiT3JcktuSfCfJriS/nuT4JNvbNbbtSZa1sl6Xkxaoqnq+qs4CVgDnJDkTeB/wq8A/B44H3tuKD9sPawbxl0iyPsmOJDv2798/za2QNJlRj9g/Bnyxqn4VeD2wC9gA3FFVq4A72mt48XW59XTX3Bi4LncucA5w9fiPAV64Lje+3JpD2yxJk6mqHwFfAdZU1d52uv1Z4G/p9k/ojrhPHVhsBfD4FPEVQ+LD3n9jVY1V1djy5ctnYYskjZsysSc5FngzcANAVf20/VNYC2xuxTbzwrU0r8tJC1CS5UmOa9PHAL8JfKftg7QzZZcAD7ZFtgCXtbNwq4GnqmovsA24IMmy9uP8AmBbm/d0ktVtXZcBn5vLbZQ0Wqv41wL7gb9N8nrgPuAq4KS2I1NVe5Oc2MoftutySdbTHdlz2mmnjVB1SQNOBjYnOYLuR/2tVfX5JF9OspzuVPpO4N+28luBi4HdwDPAuwCq6kCSDwL3tnIfqKoDbfrdwI3AMXSt4W0RL82xURL7kcDZwB9W1d1JPsYLp92HOWzX5apqI7ARYGxsbLLWvJIOUlX3A28YEj9vgvIFXDHBvE3ApiHxHcCZh1ZTSYdilGvse4A9A61nb6NL9E8MnMI7ma6V7Xj5w3JdTpIkTW7KxF5V/xd4LMmvtND5wLfprr+Nt2xfxwvX0rwuJ0nSPBm157k/BD6Z5GjgEbprbS8Dbk1yOfAD4B2trNflJEmaJyMl9qraSdd95MHOH1LW63KSJM0Te56TJKlHHARGknrCAV4EHrFLktQrJnZJknrExC5JUo+Y2CVJ6hETuyRJPWJilySpR0zskiT1iIldkqQeMbFLktQjJnZJknrExC5JUo+Y2CVJ6hETuyRJPWJil5aQJK9Ick+SbyV5KMmftfjpSe5O8nCSW5Ic3eIvb693t/krB9b1vhb/bpILB+JrWmx3kg1zvY3SUmdil5aWZ4Hzqur1wFnAmiSrgY8A11bVKuBJ4PJW/nLgyar6ZeDaVo4kZwCXAr8GrAH+MskRSY4APg5cBJwBvLOVlTRHTOzSElKdn7SXR7VHAecBt7X4ZuCSNr22vabNPz9JWvzmqnq2qr4P7AbOaY/dVfVIVf0UuLmVlTRHTOzSEtOOrHcC+4DtwPeAH1XVc63IHuCUNn0K8BhAm/8U8JrB+EHLTBSXNEdM7NISU1XPV9VZwAq6I+zXDSvWnjPBvOnGXyTJ+iQ7kuzYv3//aBWXNBITu7REVdWPgK8Aq4HjkhzZZq0AHm/Te4BTAdr8VwMHBuMHLTNR/OD33lhVY1U1tnz58tnaJEmY2KUlJcnyJMe16WOA3wR2AXcCb2/F1gGfa9Nb2mva/C9XVbX4pa3V/OnAKuAe4F5gVWtlfzRdA7sth3/LJI07cuoiknrkZGBza73+MuDWqvp8km8DNyf5EPBN4IZW/gbgE0l20x2pXwpQVQ8luRX4NvAccEVVPQ+Q5EpgG3AEsKmqHpq7zZM0UmJP8ijwNPA88FxVjSU5HrgFWAk8CvxOVT3ZWsx+DLgYeAb4/ar6RlvPOuBP22o/VFWbW/yNwI3AMcBW4Kp2VCBpFlXV/cAbhsQfobvefnD8n4B3TLCuDwMfHhLfSrcfS5oH0zkV/xtVdVZVjbXXG4A72n2vd7TX0N2/uqo91gPXA7QfAlcD59L9A7k6ybK2zPWt7Phya2a8RZIkLWGHco198P7Wg+97vandL3sXXaOck4ELge1VdaCqnqS7zWZNm3dsVX29HaXfNLAuSZI0DaMm9gK+lOS+JOtb7KSq2gvQnk9s8ene33pKmz44/hLeIiNJ0uRGbTz3pqp6PMmJwPYk35mk7GG57xW6W2SAjQBjY2Neg5ck6SAjHbFX1ePteR/wWbpr5E+00+i0532t+HTvb93Tpg+OS5KkaZoysSf5xSSvGp8GLgAe5MX3tx583+tl6awGnmqn6rcBFyRZ1hrNXQBsa/OeTrK6tai/bGBdkiRpGkY5FX8S8Nku53Ik8HdV9cUk9wK3Jrkc+AEv3BKzle5Wt910t7u9C6CqDiT5IF0HFgAfqKoDbfrdvHC72+3tIUlawFZu+MKUZR695q1zUBMNmjKxt/tbXz8k/o/A+UPiBVwxwbo2AZuGxHcAZ45QX0mSNAm7lJUkqUdM7JIk9YiJXZKkHjGxS5LUIyZ2SZJ6xMQuSVKPmNglSeoRE7skST1iYpckqUdM7JIk9YiJXZKkHjGxS0tEklOT3JlkV5KHklzV4u9P8g9JdrbHxQPLvC/J7iTfTXLhQHxNi+1OsmEgfnqSu5M8nOSWJEfP7VZKMrFLS8dzwB9V1euA1cAVSc5o866tqrPaYytAm3cp8GvAGuAvkxyR5Ajg48BFwBnAOwfW85G2rlXAk8Dlc7VxkjomdmmJqKq9VfWNNv00sAs4ZZJF1gI3V9WzVfV9uqGYz2mP3VX1SFX9FLgZWJtubOfzgNva8puBSw7P1kiaiIldWoKSrATeANzdQlcmuT/JpiTLWuwU4LGBxfa02ETx1wA/qqrnDopLmkMmdmmJSfJK4NPAe6rqx8D1wC8BZwF7gT8fLzpk8ZpBfFgd1ifZkWTH/v37p7kFkiZjYpeWkCRH0SX1T1bVZwCq6omqer6qfgb8Nd2pduiOuE8dWHwF8Pgk8R8CxyU58qD4S1TVxqoaq6qx5cuXz87GSQJM7NKS0a6B3wDsqqqPDsRPHij2NuDBNr0FuDTJy5OcDqwC7gHuBVa1FvBH0zWw21JVBdwJvL0tvw743OHcJkkvdeTURST1xJuA3wMeSLKzxf6YrlX7WXSnzR8F/gCgqh5KcivwbboW9VdU1fMASa4EtgFHAJuq6qG2vvcCNyf5EPBNuh8SkuaQiX2WrNzwhZHKPXrNWw9zTaThquprDL8OvnWSZT4MfHhIfOuw5arqEV44lS9pHngqXpKkHjGxS5LUIyZ2SZJ6ZOTE3rqS/GaSz7fXQ/uEbi1ob2l9SN/dOsIYX8e0+p2WJEnTM50j9qvouqAcN1Gf0JcDT1bVLwPXtnIz7XdakiRNw0iJPckK4K3A37TXk/UJvba9ps0/v5WfVr/Th7phkiQtRaMesf9X4D8CP2uvJ+sT+uf9SLf5T7Xy0+13+iXshlKSpMlNeR97kt8C9lXVfUneMh4eUrSmmDdRfNiPi6H9S1fVRmAjwNjY2NAyktRHo/aVIY3SQc2bgN9OcjHwCuBYuiP445Ic2Y7KB/uEHu9Hek/rM/rVwAEm7l+aSeKSJGkapjwVX1Xvq6oVVbWSrvHbl6vqd5m4T+gt7TVt/pdbH9LT6nd6VrZOkqQl5lC6lJ2oT+gbgE8k2U13pH4pzLjfaUmSNA3TSuxV9RXgK216aJ/QVfVPwDsmWH5a/U5LkqTpsec5SZJ6xMQuSVKPmNglSeoRE7skST1iYpckqUdM7JIk9YiJXZKkHjGxS0tEklOT3JlkV5KHklzV4scn2Z7k4fa8rMWT5Loku5Pcn+TsgXWta+UfTrJuIP7GJA+0Za5rIztKmkMmdmnpeA74o6p6HbAauCLJGcAG4I6qWgXc0V4DXETX9fMqYD1wPXQ/BICrgXPpOqm6evzHQCuzfmC5NXOwXZIGmNilJaKq9lbVN9r008AuuiGS1wKbW7HNwCVtei1wU3Xuohv46WTgQmB7VR2oqieB7cCaNu/Yqvp6Gx/ipoF1SZojJnZpCUqyEngDcDdwUlXthS75Aye2YqcAjw0stqfFJovvGRKXNIdM7NISk+SVwKeB91TVjycrOiRWM4gPq8P6JDuS7Ni/f/9UVZY0DSZ2aQlJchRdUv9kVX2mhZ9op9Fpz/tafA9w6sDiK4DHp4ivGBJ/iaraWFVjVTW2fPnyQ9soSS9yKMO2SlpEWgv1G4BdVfXRgVlbgHXANe35cwPxK5PcTNdQ7qmq2ptkG/CfBhrMXQC8r6oOJHk6yWq6U/yXAf/tsG9YD6zc8IX5roJ6xMQuLR1vAn4PeCDJzhb7Y7qEfmuSy4Ef8MKwy1uBi4HdwDPAuwBaAv8gcG8r94GqOtCm3w3cCBwD3N4ekuaQiV1aIqrqawy/Dg5w/pDyBVwxwbo2AZuGxHcAZx5CNSUdIq+xS5LUIyZ2SZJ6xMQuSVKPmNglSeoRE7skST1iYpckqUdM7JIk9ciUiT3JK5Lck+RbbQznP2vx05Pc3cZjviXJ0S3+8vZ6d5u/cmBd72vx7ya5cCC+psV2J9lwcB0kSdJoRjlifxY4r6peD5xFNzzjauAjwLVtDOcngctb+cuBJ6vql4FrWznauM+XAr9GN0bzXyY5IskRwMfpxn4+A3hnKytJkqZpyp7nWu9TP2kvj2qPAs4D/nWLbwbeD1xPN4bz+1v8NuAvWh/Va4Gbq+pZ4PtJdgPntHK7q+oRgNYv9Vrg24eyYZKkxWGUvvIfveatc1CTfhjpGns7st5JN+rTduB7wI+q6rlWZHDc5Z+P1dzmPwW8humP7SxJkqZppMReVc9X1Vl0wzCeA7xuWLH27BjOkiTNk2m1iq+qHwFfAVYDxyUZP5U/OO7yz8dqbvNfDRxg+mM7D3t/x3CWJGkSo7SKX57kuDZ9DPCbwC7gTuDtrdjBYziva9NvB77crtNvAS5treZPB1YB99AN/biqtbI/mq6B3ZbZ2DhJkpaaUYZtPRnY3Fqvvwy4tao+n+TbwM1JPgR8E7ihlb8B+ERrHHeALlFTVQ8luZWuUdxzwBVV9TxAkiuBbcARwKaqemjWtlCSpCVklFbx9wNvGBJ/hBdatQ/G/wl4xwTr+jDw4SHxrcDWEeorSVpERmnxrtllz3OSJPWIiV2SpB4xsUuS1CMmdmkJSbIpyb4kDw7E3p/kH5LsbI+LB+ZNa3yHicaQkDR3TOzS0nIj3VgNB7u2qs5qj60w4/EdJhpDQtIcMbFLS0hVfZXuNtRR/Hx8h6r6PjA+vsM5tPEdquqnwM3A2jYmxHl0Y0RAN4bEJbO6AZKmZGKXBHBlkvvbqfplLTbd8R1ew8RjSEiaIyZ2SdcDv0Q3LPNe4M9b3HEfpEXIxC4tcVX1RBvo6WfAX/NCx1PTHd/hh0w8hsTB7+m4D9JhYmKXlrgkJw+8fBsw3mJ+WuM7tDEhJhpDQtIcGaWveEk9keRTwFuAE5LsAa4G3pLkLLrT5o8CfwAzHt/hvQwfQ0LSHDGxS0tIVb1zSHjC5Dvd8R0mGkNC0tzxVLwkST1iYpckqUc8FT/HRhnC8NFr3joHNZEk9ZFH7JIk9YiJXZKkHjGxS5LUI15jl6TDZJQ2NdJs84hdkqQeMbFLktQjJnZJknrExC5JUo9MmdiTnJrkziS7kjyU5KoWPz7J9iQPt+dlLZ4k1yXZneT+JGcPrGtdK/9wknUD8TcmeaAtc12SYeM6S5KkKYxyxP4c8EdV9TpgNXBFkjOADcAdVbUKuKO9BriIbnjHVcB64HrofgjQjSR1Lt0gEVeP/xhoZdYPLLfm0DdNkqSlZ8rEXlV7q+obbfppYBdwCrAW2NyKbQYuadNrgZuqcxdwXBvv+UJge1UdqKonge3Amjbv2Kr6ehvP+aaBdUmSpGmY1jX2JCuBNwB3AydV1V7okj9wYit2CvDYwGJ7Wmyy+J4hcUmSNE0jJ/YkrwQ+Dbynqn48WdEhsZpBfFgd1ifZkWTH/v37p6qyJElLzkiJPclRdEn9k1X1mRZ+op1Gpz3va/E9wKkDi68AHp8ivmJI/CWqamNVjVXV2PLly0epuiRJS8ooreID3ADsqqqPDszaAoy3bF8HfG4gfllrHb8aeKqdqt8GXJBkWWs0dwGwrc17Osnq9l6XDaxLkiRNwyh9xb8J+D3ggSQ7W+yPgWuAW5NcDvwAeEebtxW4GNgNPAO8C6CqDiT5IHBvK/eBqjrQpt8N3AgcA9zeHpIkaZqmTOxV9TWGXwcHOH9I+QKumGBdm4BNQ+I7gDOnqoukQ5NkE/BbwL6qOrPFjgduAVYCjwK/U1VPtjNoH6P7of4M8Pvjd8i0fij+tK32Q1W1ucXfyAs/0rcCV7X/CZLmiD3PSUvLjby0nwj7pJB6xMQuLSFV9VXgwEFh+6SQesTELsk+KaQeGaXx3KK1csMX5rsK0mJ2WPukoDtlz2mnnTbT+kkawiN2SfZJIfWIiV2SfVJIPdLrU/GSXizJp4C3ACck2UPXut0+KaQeMbFLS0hVvXOCWfZJIfWEp+IlSeoRE7skST3iqfgFaJTb9B695q1zUBNJ0mLjEbskST1iYpckqUdM7JIk9YiJXZKkHjGxS5LUIyZ2SZJ6xMQuSVKPmNglSeoRO6iRJC14o3TcBXbeBR6xS5LUKyZ2SZJ6xMQuSVKPTJnYk2xKsi/JgwOx45NsT/Jwe17W4klyXZLdSe5PcvbAMuta+YeTrBuIvzHJA22Z65JktjdSkqSlYpQj9huBNQfFNgB3VNUq4I72GuAiYFV7rAeuh+6HAHA1cC5wDnD1+I+BVmb9wHIHv5ckSRrRlIm9qr4KHDgovBbY3KY3A5cMxG+qzl3AcUlOBi4EtlfVgap6EtgOrGnzjq2qr1dVATcNrEuSJE3TTK+xn1RVewHa84ktfgrw2EC5PS02WXzPkLgkSZqB2W48N+z6eM0gPnzlyfokO5Ls2L9//wyrKGmYJI+29i47k+xosVlrTyNpbsy0g5onkpxcVXvb6fR9Lb4HOHWg3Arg8RZ/y0Hxr7T4iiHlh6qqjcBGgLGxsQl/AEiasd+oqh8OvB5vT3NNkg3t9Xt5cXuac+naypw70J5mjO5H+n1JtrRLcL0yaocpmluj/F363onNTI/YtwDjv8TXAZ8biF/Wfs2vBp5qp+q3ARckWdZ+8V8AbGvznk6yurWGv2xgXZLm36y0p5nrSktL2ZRH7Ek+RXe0fUKSPXS/xq8Bbk1yOfAD4B2t+FbgYmA38AzwLoCqOpDkg8C9rdwHqmq8Qd676VreHwPc3h6S5l4BX0pSwH9vZ8he1J4myUzb00iaI1Mm9qp65wSzzh9StoArJljPJmDTkPgO4Myp6iHpsHtTVT3ekvf2JN+ZpOwhtZtJsp7uNldOO+20mdRV0gTseU4SAFX1eHveB3yWrs+JJ9opdqbRnmZY/OD32lhVY1U1tnz58tneFGlJM7FLIskvJnnV+DRdO5gHmaX2NHO4KdKS57CtkgBOAj7benQ+Evi7qvpiknuZvfY0kuaAiX2R8pYOzaaqegR4/ZD4PzJL7WkkzQ1PxUuS1CMmdkmSesTELklSj5jYJUnqERO7JEk9YmKXJKlHTOySJPWIiV2SpB4xsUuS1CP2PNdjo/ROB/ZQJ0l94hG7JEk9YmKXJKlHPBUvB5SRpB7xiF2SpB4xsUuS1COeipckLSl9v/zoEbskST3iEbtG0vdfuJLUFx6xS5LUIwvmiD3JGuBjwBHA31TVNfNcJUkz0Id9edReG6WFaEEk9iRHAB8H/iWwB7g3yZaq+vb81kzSdByufXk2u0c2aavvFkRiB84BdlfVIwBJbgbWAiZ2aXGZ133ZpC0tnMR+CvDYwOs9wLnzVBdJM+e+rF6YzR+Jc92weKEk9gyJ1UsKJeuB9e3lT5J8d2D2CcAPD0Pd5sJirju0+ucj812NGVnMn/0odf9nc1GRAbOxL8+Hxfw9GOR2LCyz/b9xpP15oST2PcCpA69XAI8fXKiqNgIbh60gyY6qGjs81Tu8FnPdYXHX37rPukPel+fDAv0sp83tWFjmazsWyu1u9wKrkpye5GjgUmDLPNdJ0vS5L0vzbEEcsVfVc0muBLbR3SKzqaoemudqSZom92Vp/i2IxA5QVVuBrYewigVzWm8GFnPdYXHX37rPslnYl+fDgvwsZ8DtWFjmZTtS9ZJ2LZIkaZFaKNfYJUnSLOhFYk+yJsl3k+xOsmEe6/FokgeS7Eyyo8WOT7I9ycPteVmLJ8l1rc73Jzl7YD3rWvmHk6wbiL+xrX93W3bYrUXTqe+mJPuSPDgQO+z1neg9ZqHu70/yD+3z35nk4oF572v1+G6SCwfiQ787rfHX3a2Ot7SGYCR5eXu9u81fOYO6n5rkziS7kjyU5KrJPpeF9tn3TZKrkjzY/hbvme/6jGo6++9CNsF2vKP9PX6WZFG0jp9gO/5Lku+0/fazSY6bk8pU1aJ+0DXQ+R7wWuBo4FvAGfNUl0eBEw6K/WdgQ5veAHykTV8M3E533+9q4O4WPx54pD0va9PL2rx7gF9vy9wOXHSI9X0zcDbw4FzWd6L3mIW6vx/490PKntG+Fy8HTm/flyMm++4AtwKXtum/At7dpv8d8Fdt+lLglhnU/WTg7Db9KuDvWx0XxWffpwdwJvAg8At0bY7+J7Bqvus1Yt1H3n8X8mOC7Xgd8CvAV4Cx+a7jIWzHBcCRbfojc/X36MMR+8+7sKyqnwLjXVguFGuBzW16M3DJQPym6twFHJfkZOBCYHtVHaiqJ4HtwJo279iq+np135KbBtY1I1X1VeDAPNR3ovc41LpPZC1wc1U9W1XfB3bTfW+Gfnfa0e15wG0TfA7jdb8NOH+6Z06qam9VfaNNPw3souuxbVF89j3zOuCuqnqmqp4D/hfwtnmu00imuf8uWMO2o6p2VdV8d1o0LRNsx5fa9wrgLrp+HQ67PiT2YV1YnjJPdSngS0nuS9ezFsBJVbUXun/owIktPlG9J4vvGRKfbXNR34neYzZc2U57bRo4DTndur8G+NHADjlY958v0+Y/1crPSDuV/wbgbhb/Z78YPQi8OclrkvwC3dmRU6dYZiHz77tw/Ru6s2eHXR8S+0hdWM6RN1XV2cBFwBVJ3jxJ2YnqPd34XFkM9b0e+CXgLGAv8OctPpt1n7XtSvJK4NPAe6rqx5MVneA9F9JnvyhV1S66U6TbgS/SXY55btKFpGlK8id036tPzsX79SGxj9SF5Vyoqsfb8z7gs3Snep9op0Zpz/ta8YnqPVl8xZD4bJuL+k70Hoekqp6oquer6mfAX9N9/jOp+w/pTncfeVD8Retq81/N6JcEfi7JUXRJ/ZNV9ZkWXrSf/WJWVTdU1dlV9Wa6v+XD812nQ+Dfd4FpjVp/C/jddmnssOtDYl8QXVgm+cUkrxqfpms08WCry3hr5XXA59r0FuCy1uJ5NfBUO3W2DbggybJ2KvkCYFub93SS1e2a7mUD65pNc1Hfid7jkIz/Q2veRvf5j7/fpelatJ8OrKJrXDb0u9N2vjuBt0/wOYzX/e3Al6e7s7bP4wZgV1V9dGDWov3sF7MkJ7bn04B/BXxqfmt0SPz7LiBJ1gDvBX67qp6ZszeeixZ6h/tBd13s7+laOP/JPNXhtXSn8b4FPDReD7rrr3fQHQXcARzf4gE+3ur8AAMtP+muxexuj3cNxMfoktX3gL+gdTB0CHX+FN0p6/9Hd5R3+VzUd6L3mIW6f6LV7X66f3AnD5T/k1aP7zJwN8FE353297ynbdP/AF7e4q9or3e3+a+dQd3/Bd2p8fuBne1x8WL57Pv2AP433Xjx3wLOn+/6TKPeI++/C/kxwXa8rU0/CzxB94N13us6g+3YTdcOZnw//6u5qIs9z0mS1CN9OBUvSZIaE7skST1iYpckqUdM7JIk9YiJXZKkHjGxS5LUIyZ2SZJ6xMQuSVKP/H+9KhGj6EW1ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data = data.head(3)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')\n",
    "\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['Log1pSalary'], bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244768,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Log1pSalary'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(data[\"SalaryNormalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(data[\"SalaryNormalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.206078"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(data[\"Log1pSalary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.517393"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(data[\"Log1pSalary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to predict one number, __Log1pSalary__. (log(1 + 200000))\n",
    "\n",
    "Log1pSalary 예측이 임무\n",
    "\n",
    "Title : Job position  \n",
    "FullDescription : 실제 할일  \n",
    "LocationRaw : 위치 (Detail)  \n",
    "LocationNormalized : 위치  \n",
    "Contract Type : 계약 유형  \n",
    "Contract Time : 계약 시간  \n",
    "Company : 회사  \n",
    "Category : 범주  \n",
    "SalaryRaw : 급여 범위 및 추가 속성  \n",
    "SalaryNormalized : 급여 평균  \n",
    "SourceName : 출처  \n",
    "\n",
    "To do so, our model can access a number of features:\n",
    "* Free text: __`Title`__ and  __`FullDescription`__\n",
    "* Categorical: __`Category`__, __`Company`__, __`LocationNormalized`__, __`ContractType`__, and __`ContractTime`__.\n",
    "\n",
    "dropna함수는 column내에 NaN값이 있으면 해당 내용은 필요없다 간주하고 삭제해버린다.  \n",
    "\n",
    "fillna함수도 굉장히 유용한다 NaN을 특정 값으로 대체하는 기능을 한다.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182771</th>\n",
       "      <td>71614858</td>\n",
       "      <td>Registered General Nurse (RGN)  East London  C...</td>\n",
       "      <td>Registered General Nurse/RN/RGNLocation: East ...</td>\n",
       "      <td>Chingford</td>\n",
       "      <td>Chingford</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HC Recruitment Services</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>13.00 - 13.15/Hour</td>\n",
       "      <td>25104</td>\n",
       "      <td>staffnurse.com</td>\n",
       "      <td>10.130822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44035</th>\n",
       "      <td>68506863</td>\n",
       "      <td>Buyer  Menswear</td>\n",
       "      <td>Buyer  Menswear : The Client This design led c...</td>\n",
       "      <td>North London London South East</td>\n",
       "      <td>North Lambeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>FASHION &amp; RETAIL PERSONNEL LIMITED</td>\n",
       "      <td>Retail Jobs</td>\n",
       "      <td>40000 - 45000 per annum</td>\n",
       "      <td>42500</td>\n",
       "      <td>retailchoice.com</td>\n",
       "      <td>10.657283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101855</th>\n",
       "      <td>69547809</td>\n",
       "      <td>HGV 2 Moffett Driver</td>\n",
       "      <td>We have a breathtaking opportunity for a HGV C...</td>\n",
       "      <td>Southall</td>\n",
       "      <td>Southall</td>\n",
       "      <td>full_time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HR Go Recruitment</td>\n",
       "      <td>Logistics &amp; Warehouse Jobs</td>\n",
       "      <td>8.50 - 12.75 per hour</td>\n",
       "      <td>20400</td>\n",
       "      <td>Jobcentre Plus</td>\n",
       "      <td>9.923339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                                              Title  \\\n",
       "182771  71614858  Registered General Nurse (RGN)  East London  C...   \n",
       "44035   68506863                                    Buyer  Menswear   \n",
       "101855  69547809                               HGV 2 Moffett Driver   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "182771  Registered General Nurse/RN/RGNLocation: East ...   \n",
       "44035   Buyer  Menswear : The Client This design led c...   \n",
       "101855  We have a breathtaking opportunity for a HGV C...   \n",
       "\n",
       "                           LocationRaw LocationNormalized ContractType  \\\n",
       "182771                       Chingford          Chingford    full_time   \n",
       "44035   North London London South East      North Lambeth          NaN   \n",
       "101855                        Southall           Southall    full_time   \n",
       "\n",
       "       ContractTime                             Company  \\\n",
       "182771          NaN             HC Recruitment Services   \n",
       "44035     permanent  FASHION & RETAIL PERSONNEL LIMITED   \n",
       "101855          NaN                   HR Go Recruitment   \n",
       "\n",
       "                          Category                SalaryRaw  SalaryNormalized  \\\n",
       "182771   Healthcare & Nursing Jobs       13.00 - 13.15/Hour             25104   \n",
       "44035                  Retail Jobs  40000 - 45000 per annum             42500   \n",
       "101855  Logistics & Warehouse Jobs    8.50 - 12.75 per hour             20400   \n",
       "\n",
       "              SourceName  Log1pSalary  \n",
       "182771    staffnurse.com    10.130822  \n",
       "44035   retailchoice.com    10.657283  \n",
       "101855    Jobcentre Plus     9.923339  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "target_column = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text data\n",
    "\n",
    "Just like last week, applying NLP to a problem begins from tokenization: splitting raw text into sequences of tokens (words, punctuation(구두법), etc).\n",
    "\n",
    "__Your task__ is to lowercase and tokenize all texts under `Title` and `FullDescription` columns. Store the tokenized data as a __space-separated__ string of tokens for performance reasons.\n",
    "\n",
    "It's okay to use nltk tokenizers. Assertions were designed for WordPunctTokenizer, slight deviations are okay.\n",
    "\n",
    "\n",
    "regexp를 사용하여 텍스트를 영문자 및 비영 문자의 순서로 토큰화    \n",
    "\n",
    "\\w+|[^\\w\\s]+.  \n",
    "\n",
    "<p> from nltk.tokenize import WordPunctTokenizer  \n",
    "<p> s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\" \n",
    "\n",
    "<p> WordPunctTokenizer().tokenize(s) \n",
    "<p> ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', \n",
    "<p> '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']  \n",
    "\n",
    "\n",
    "\n",
    "http://excelsior-cjh.tistory.com/63  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "2         Mathematical Modeller / Simulation Analyst / O...\n",
      "100002    A successful and high achieving specialist sch...\n",
      "200002    Web Designer  HTML, CSS, JavaScript, Photoshop...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw text:\")\n",
    "print(data[\"FullDescription\"][2::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JunChangWook/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "\n",
    "index = 0\n",
    "for item in data[\"FullDescription\"]:\n",
    "    data[\"FullDescription\"][index] = tokenizer.tokenize(item)\n",
    "    index = index + 1\n",
    "    \n",
    "index = 0\n",
    "for item in data[\"Title\"]:\n",
    "    data[\"Title\"][index] = tokenizer.tokenize(item)\n",
    "    index = index + 1\n",
    "# see task above\n",
    "#<YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assume that our text is a space-separated list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized:\")\n",
    "print(data[\"FullDescription\"][2::100000])\n",
    "assert data[\"FullDescription\"][2][:50] == 'mathematical modeller / simulation analyst / opera'\n",
    "assert data[\"Title\"][54321] == 'international digital account manager ( german )'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times.  \n",
    "모든 단어가 똑같이 유용하지 않다. 몇몇 단어는 오타 또는 희귀 단어 이다.\n",
    "\n",
    "Let's count how many times is each word present in the data so that we can build a \"white list\" of known words.  \n",
    "단어 카운트를 기반으로 유용한 단어 리스트를 만든다. (white lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
    "# build a dictionary { token -> it's count }\n",
    "import collections\n",
    "\n",
    "dictionary = []\n",
    "\n",
    "for item in data[\"FullDescription\"]:\n",
    "    dictionary.extend(item)\n",
    "\n",
    "for item in data[\"Title\"]:\n",
    "    dictionary.extend(item)\n",
    "\n",
    "token_counts = collections.Counter(dictionary)\n",
    "#token_counts = <YOUR CODE>\n",
    "\n",
    "# hint: you may or may not want to use collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))\n",
    "\n",
    "#assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "#assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many words are there for each count\n",
    "plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now filter tokens a list of all tokens that occur at least 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "temp_tokens = []\n",
    "\n",
    "#for k,v in token_counts.items():\n",
    "    #if v > min_count:\n",
    "    #temp_tokens.append(k)\n",
    "\n",
    "for k,v in token_counts.items():\n",
    "    if v > min_count:\n",
    "        temp_tokens.append(k)\n",
    "\n",
    "tokens = temp_tokens\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "# tokens = <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + sorted(tokens)\n",
    "print(\"Vocabulary size:\", len(tokens))\n",
    "\n",
    "assert type(tokens) == list\n",
    "assert len(tokens) in range(32000, 35000)\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_to_id = <your code here>\n",
    "token_to_id = {word: idx for idx, word in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into neural network-digestible matrices.\n",
    "행렬로 매핑하기  \n",
    "\n",
    ">>> a = list(map(str, range(10)))  \n",
    ">>> a  \n",
    "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "# 매트릭스 구조를 만들고 있다.\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    # object , classinfo 같으면 참 아니면 거짓\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "    # 처음 한번은 양의 무한대와 비교하고 나머지는 시컨스 max_len와 비교 한다.    \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    # 전체를 패드로 만들고\n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    # 사전에 있으면 그 인텍스를 아니면 UNK_IX를 넣어서 매트릭스를 구성하고 있다.\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "        print(matrix)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lines:\")\n",
    "print('\\n'.join(data[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement more advanced encodings: tf-idf, pseudo-time-series, etc.\n",
    "\n",
    "one-hot 인코딩을 사용   Advanced encoding : tf-idf  \n",
    "\n",
    ">>> list(zip([1, 2, 3], [4, 5, 6]))  \n",
    "[(1, 4), (2, 5), (3, 6)]  \n",
    "\n",
    "set 순서가 없는 딕셔너리 만들기  \n",
    "  \n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "corpus = [  \n",
    "    'This is the first document.',  \n",
    "    'This is the second second document.',  \n",
    "    'And the third one.',\n",
    "    'Is this the first document?',  \n",
    "    'The last document?',      \n",
    "]\n",
    "vect = CountVectorizer()  \n",
    "vect.fit(corpus)  \n",
    "vect.vocabulary_  \n",
    "  \n",
    "\n",
    "{'this': 9,  \n",
    " 'is': 3,  \n",
    " 'the': 7,  \n",
    " 'first': 2,  \n",
    " 'document': 1,  \n",
    " 'second': 6,  \n",
    " 'and': 0,  \n",
    " 'third': 8,  \n",
    " 'one': 5,  \n",
    " 'last': 4}  \n",
    "   \n",
    " \n",
    " vect.transform(['This is the second document.']).toarray()  \n",
    " \n",
    " array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])  \n",
    " \n",
    " vect.transform(['Something completely new.']).toarray()  \n",
    " \n",
    " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])  \n",
    " \n",
    " vect.transform(corpus).toarray()  \n",
    " \n",
    " array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],  \n",
    "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],  \n",
    "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],  \n",
    "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],  \n",
    "       [0, 1, 0, 0, 1, 0, 0, 1, 0, 0]])  \n",
    "       \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*collections.Counter(data['Company']).most_common(1000)) # 동일한 위치 묶어준다.\n",
    "print(top_companies)\n",
    "recognized_companies = set(top_companies)\n",
    "print(recognized_companies)\n",
    "# top 1000개 이상은 Company를 표현하고 아닌 모든 것들은 Other로 처리 한다. 여기에 pandas apply 함수를 통해 수행한다.\n",
    "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "# dict를 y 축으로 묶는다. pandas apply 함수를 통해서 수행한다. \n",
    "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The deep learning part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment. (토큰을 배웠고 이제 기계학습 실험)\n",
    "\n",
    "As before, we won't focus too much on validation, opting for a simple train-test split. (학습 훈련 검증 셋 분할)\n",
    "\n",
    "__To be completely rigorous,__ we've comitted a small crime here: we used the whole data for tokenization and vocabulary building. A more strict way would be to do that part on training set only. You may want to do that and measure the magnitude of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 구성하는 함수\n",
    "def make_batch(data, max_len=None, word_dropout=0):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data. (케라스 친화적으로 만든다)\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability (word_dropout 확률로 UNK_IX로 대체)\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len] (배치 사이즈, 타이틀 최대 크기) 매트릭스 구성\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    # target_column = \"Log1pSalary\"\n",
    "    if target_column in data.columns:\n",
    "        batch[target_column] = data[target_column].values\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    # 변환 해준다. 모든 부분의 full_like \n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)]) # matrix를 replace_with로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_batch(data_train[:3], max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "Our basic model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary. (급여 예측에 3개의 특성을 쓴다.)\n",
    "\n",
    "<img src=\"https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly doesn't fit into keras' __Sequential__ interface. To build such a network, one will have to use __[Keras Functional API](https://keras.io/models/model/)__.\n",
    "\n",
    "https://keras.io/layers/merge/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#from keras.models import Sequential\n",
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "    \"\"\" Build a model that maps three data sources to a single linear output: predicted log1p(salary) \"\"\"\n",
    "    \n",
    "    l_title = L.Input(shape=[None], name=\"Title\")\n",
    "    l_descr = L.Input(shape=[None], name=\"FullDescription\")\n",
    "    l_categ = L.Input(shape=[n_cat_features], name=\"Categorical\")\n",
    "    \n",
    "    # Build your monster!\n",
    "    \n",
    "    x1 = keras.layers.Dense(8, activation='relu')(l_title)\n",
    "    x2 = keras.layers.Dense(8, activation='relu')(l_descr)\n",
    "    x3 = keras.layers.Dense(8, activation='relu')(l_categ)\n",
    "    added = keras.layers.add([x1, x2, x3])\n",
    "\n",
    "    # <YOUR CODE>\n",
    "    output_layer = keras.layers.Dense(1)(added)\n",
    "    #output_layer = <...>\n",
    "    # end of your code\n",
    "    \n",
    "    model = keras.models.Model(inputs=[l_title, l_descr, l_categ], outputs=[output_layer])\n",
    "    model.compile('adam', 'mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary() # 모델 요약\n",
    "\n",
    "dummy_pred = model.predict(make_batch(data_train[:100]))\n",
    "dummy_loss = model.train_on_batch(make_batch(data_train[:100]), data_train['Log1pSalary'][:100])[0]\n",
    "assert dummy_pred.shape == (100, 1)\n",
    "assert len(np.unique(dummy_pred)) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert np.ndim(dummy_loss) == 0 and 0. <= dummy_loss <= 250., \"make sure you minimize MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation\n",
    "\n",
    "As usual, we gonna feed our monster with random minibatches of data. \n",
    "미니 배치 사용  \n",
    "\n",
    "As we train, we want to monitor not only loss function, which is computed in log-space, but also the actual error measured in dollars.\n",
    "\n",
    "로그 공간에서 계산 된 손실 함수뿐만 아니라 달러로 측정 한 실제 오차를 모니터링하려고 합니다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], **kwargs)\n",
    "            target = batch.pop(target_column)\n",
    "            yield batch, target\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We can now fit our model the usual minibatch way. The interesting part is that we train on an infinite stream of minibatches, produced by `iterate_minibatches` function. (iterate_minibatches 함수는 무한 미니 버스 스트림 구성.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10            # definitely too small\n",
    "steps_per_epoch = 100  # for full pass over data: (len(data_train) - 1) // batch_size + 1\n",
    "\n",
    "model = build_model()\n",
    "#배치 별로 모델 트레이닝\n",
    "model.fit_generator(iterate_minibatches(data_train, batch_size, cycle=True, word_dropout=0.05), \n",
    "                    epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
    "                    \n",
    "                    validation_data=iterate_minibatches(data_val, batch_size, cycle=True),\n",
    "                    validation_steps=data_val.shape[0] // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=batch_size, name=\"\", **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    for batch_x, batch_y in iterate_minibatches(data, batch_size=batch_size, shuffle=False, **kw):\n",
    "        batch_pred = model.predict(batch_x)[:, 0]\n",
    "        squared_error += np.sum(np.square(batch_pred - batch_y))\n",
    "        abs_error += np.sum(np.abs(batch_pred - batch_y))\n",
    "        num_samples += len(batch_y)\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % (squared_error / num_samples))\n",
    "    print(\"Mean absolute error: %.5f\" % (abs_error / num_samples))\n",
    "    return squared_error, abs_error\n",
    "    \n",
    "print_metrics(model, data_train, name='Train')\n",
    "print_metrics(model, data_val, name='Val');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus part: explaining model predictions\n",
    "\n",
    "It's usually a good idea to understand how your model works before you let it make actual decisions. It's simple for linear models: just see which words learned positive or negative weights. However, its much harder for neural networks that learn complex nonlinear dependencies.\n",
    "선형 모델은 비선형 모델 보다 쉽다고 이야기 하고 있음\n",
    "\n",
    "There are, however, some ways to look inside the black box:\n",
    "블랙 박스 들여다 보는 방법\n",
    "* Seeing how model responds to input perturbations  \n",
    "입력에 대해서 모델이 어떻게 응답하는지 본다.  \n",
    "* Finding inputs that maximize/minimize activation of some chosen neurons (_read more [on distill.pub](https://distill.pub/2018/building-blocks/)_)  \n",
    "활성화된 뉴럴의 선택해 최대/최소 찾기\n",
    "* Building local linear approximations to your neural network: [article](https://arxiv.org/abs/1602.04938), [eli5 library](https://github.com/TeamHG-Memex/eli5/tree/master/eli5/formatters)  \n",
    "신경망에 대한 로컬 선형 근사법 작성  \n",
    "Today we gonna try the first method just because it's the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(model, sample, col_name='Title'):\n",
    "    \"\"\" Computes the effect each word had on model predictions \"\"\"\n",
    "    sample = dict(sample)\n",
    "    sample_col_tokens = [tokens[token_to_id.get(tok, 0)] for tok in sample[col_name].split()]\n",
    "    data_drop_one_token = pd.DataFrame([sample] * (len(sample_col_tokens) + 1))\n",
    "\n",
    "    for drop_i in range(len(sample_col_tokens)):\n",
    "        data_drop_one_token.loc[drop_i, col_name] = ' '.join(UNK if i == drop_i else tok\n",
    "                                                   for i, tok in enumerate(sample_col_tokens)) \n",
    "\n",
    "    *predictions_drop_one_token, baseline_pred = model.predict(make_batch(data_drop_one_token))[:, 0]\n",
    "    diffs = baseline_pred - predictions_drop_one_token\n",
    "    return list(zip(sample_col_tokens, diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display_html\n",
    "\n",
    "def draw_html(tokens_and_weights, cmap=plt.get_cmap(\"bwr\"), display=True,\n",
    "              token_template=\"\"\"<span style=\"background-color: {color_hex}\">{token}</span>\"\"\",\n",
    "              font_style=\"font-size:14px;\"\n",
    "             ):\n",
    "    \n",
    "    def get_color_hex(weight):\n",
    "        rgba = cmap(1. / (1 + np.exp(weight)), bytes=True)\n",
    "        return '#%02X%02X%02X' % rgba[:3]\n",
    "    \n",
    "    tokens_html = [\n",
    "        token_template.format(token=token, color_hex=get_color_hex(weight))\n",
    "        for token, weight in tokens_and_weights\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    raw_html = \"\"\"<p style=\"{}\">{}</p>\"\"\".format(font_style, ' '.join(tokens_html))\n",
    "    if display:\n",
    "        display_html(HTML(raw_html))\n",
    "        \n",
    "    return raw_html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 36605\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 12077\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(len(data))\n",
    "print(\"Index:\", i)\n",
    "print(\"Salary (gbp):\", np.expm1(model.predict(make_batch(data.iloc[i: i+1]))[0, 0]))\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"Title\")\n",
    "draw_html([(tok, weight * 5) for tok, weight in tokens_and_weights], font_style='font-size:20px;');\n",
    "\n",
    "tokens_and_weights = explain(model, data.loc[i], \"FullDescription\")\n",
    "draw_html([(tok, weight * 10) for tok, weight in tokens_and_weights]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Terrible start-up idea #1962:__ make a tool that automaticaly rephrases your job description (or CV) to meet salary expectations :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
